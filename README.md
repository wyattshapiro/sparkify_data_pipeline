
# Data Pipeline with Airflow for Sparkify

## Goal

Sparkify has been collecting on songs and user activity for their new music streaming app. One of their teams is interested in analyzing their users listening habits, which could help them classify their users for targeted ads, create/improve a recommendation algorithm for songs, and more. They want to introduce more automation and monitoring to their data warehouse ETL pipelines, so have chosen Airflow.

Note: This project is for a fictional company and is part of Udacity's Data Engineering Nanodegree.

### Problem

Currently, Sparkify's user history and song data is stored on S3 in a directory of JSON log formats.
This format does not lend itself well to analysis.

In addition, data quality and pipeline monitoring are not available, which makes it hard to understand data lineage and errors that may occur.

### Solution

In order to enable a team to effectively gain insight from user activity, a data engineer needs to create a data pipeline that can be scheduled, monitored, and backfilled. The proposed plan consists of a Python ETL pipeline that will:

- Extract each JSON file hosted on S3.
- Load data into staging tables on Redshift.
- Transform and load data into analytics tables with star schema on Redshift.
- Perform data quality checks.

![Alt text](sparkify_DAG.png?raw=true "Sparkify Data Pipeline DAG")


## Data

The source data resides in S3 and needs to be processed in Sparkify's data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.

### Song Dataset

- This dataset is a subset of real song data from the Million Song Dataset.
- Files live on S3 with the link s3://udacity-dend/song_data
- Each file is in JSON format and contains metadata about a song and the artist of that song.
  - The files are partitioned by the first three letters of each song's track ID. For example:
    - song_data/A/B/C/TRABCEI128F424C983.json
    - song_data/A/A/B/TRAABJL12903CDCF1A.json

### Log Dataset

- This dataset is event data generated by an event simulator based on the songs in the dataset above.
- Files live on S3 with the link s3://udacity-dend/log_data
- Each file is in JSON format and contains data about user events in the app.
  - The files are partitioned by year and month. For example:
    - log_data/2018/11/2018-11-12-events.json
    - log_data/2018/11/2018-11-13-events.json


## Data Models

### Entities

The database is structured as a star schema for analysis of song plays. As such, the fact table (ie center of the star) will be songplays, and it will have it's associated dimensions related as foreign keys.

Fact table
- songplays: records in log data associated with song plays

Dimension tables
- app_users: users in the app
- songs: songs in music database
- artists: artists in music database
- time: timestamps of records in songplays broken down into specific units

### Entity Relationship Diagram (ERD)

![Alt text](sparkify_ERD.png?raw=true "Sparkify ERD")


## Installation

Clone the repo onto your machine with the following command:

$ git checkout https://github.com/wyattshapiro/sparkify_date_pipeline.git


## Dependencies

I use Python 3.7.

See https://www.python.org/downloads/ for information on download.

----

I use virtualenv to manage dependencies, if you have it installed you can run
the following commands from the root code directory to create the environment and
activate it:

$ python3 -m venv venv

$ source venv/bin/activate

See https://virtualenv.pypa.io/en/stable/ for more information.

----

I use pip to install dependencies, which comes installed in a virtualenv.
You can run the following to install dependencies:

$ pip install -r requirements.txt

See https://pip.pypa.io/en/stable/installing/ for more information.

----

I use AWS S3 and Redshift for data storage and processing.

See https://aws.amazon.com/ for more information.

----

I use Apache Airflow to orchestrate and schedule tasks.

There are several main directories for Airflow:
- dags/: contains all DAGs (Directed Acyclic Graph)
- plugins/: contains all customizable code that can be leveraged by DAGs
- logs/: contains all log files that track code execution

See https://airflow.apache.org/ for more information.


## Usage

**Steps to run**
1. Navigate to top of project directory
2. Create virtualenv (see Dependencies)
3. Activate virtualenv (see Dependencies)
4. Install requirements (see Dependencies)
5. Start up Redshift cluster
6. $ airflow webserver
7. $ airflow scheduler
8. Configure Airflow connections to AWS and Redshift cluster
9. Trigger DAG in Airflow UI
